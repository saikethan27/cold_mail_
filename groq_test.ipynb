{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dc8f155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='While it is possible to use AI techniques to assist with web scraping, the actual process of scraping data from websites typically involves a series of commands using programming languages such as Python or tools like BeautifulSoup and Selenium. Here\\'s a general outline of the steps involved in web scraping using these tools:\\n\\n1. **Identify the website**: Decide on the website from which you want to extract data.\\n2. **Inspect the website**: Use your web browser\\'s developer tools to inspect the HTML structure of the website and identify the specific elements containing the data you want.\\n3. **Install necessary libraries**: If you\\'re using Python, install the required libraries such as BeautifulSoup and Requests using pip:\\n\\n   ```\\n   pip install beautifulsoup4 requests\\n   ```\\n   \\n   You might also need to install Selenium if the website relies heavily on JavaScript:\\n\\n   ```\\n   pip install selenium\\n   ```\\n\\n4. **Import libraries**: In your Python script, import the necessary libraries:\\n\\n   ```python\\n   import requests\\n   from bs4 import BeautifulSoup\\n   from selenium import webdriver\\n   ```\\n\\n5. **Send a request**: Send an HTTP request to the website using the Requests library:\\n\\n   ```python\\n   url = \"https://example.com\"\\n   response = requests.get(url)\\n   ```\\n\\n6. **Parse the HTML**: Parse the HTML content of the response using BeautifulSoup:\\n\\n   ```python\\n   soup = BeautifulSoup(response.text, \"html.parser\")\\n   ```\\n\\n7. **Extract data**: Use the parsed HTML to extract the data you need. This typically involves finding the specific HTML elements containing the data and extracting their text or attribute values:\\n\\n   ```python\\n   data = soup.find_all(\"div\", class_=\"data-class\")\\n   extracted_data = [item.text for item in data]\\n   ```\\n\\n8. **Handle JavaScript**: If the website relies heavily on JavaScript, you might need to use Selenium to render the website and extract the data:\\n\\n   ```python\\n   driver = webdriver.Firefox()\\n   driver.get(url)\\n   data = driver.find_elements_by_class_name(\"data-class\")\\n   extracted_data = [item.text for item in data]\\n   driver.quit()\\n   ```\\n\\n9. **Store the data**: Store the extracted data in a file or database for further processing or analysis.\\n\\nWhile this process doesn\\'t directly involve AI, you could potentially use AI techniques to automate some of these steps, such as identifying the website and inspecting its HTML structure. However, this typically requires a more advanced understanding of web scraping and AI techniques.' response_metadata={'token_usage': {'completion_time': 0.99659457, 'completion_tokens': 615, 'prompt_time': 0.002447043, 'prompt_tokens': 19, 'queue_time': 0.01298346, 'total_time': 0.999041613, 'total_tokens': 634}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None} id='run-47c1c6de-dcb2-4f5b-912d-da00d7d91085-0' usage_metadata={'input_tokens': 19, 'output_tokens': 615, 'total_tokens': 634}\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    "    temperature=0,\n",
    "    groq_api_key='<API key>'\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "response=llm.invoke('how to web scrape with ai with series of commands')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bda57956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software Development Engineer, EFA - Job ID: 2713915 | Amazon.jobs\n",
      "Skip to main contentHomeYour job applicationAmazon culture & benefitsDiversity at AmazonLocationsTeamsJob categoriesResourcesInterview tipsDisability accommodationsAbout AmazonFAQ×Software Development Engineer, EFAJob ID: 2713915 | Annapurna Labs LTDApply nowDESCRIPTIONAWS Utility Computing (UC) provides product innovations — from foundational services such as Amazon’s Simple Storage Service (S3) and Amazon Elastic Compute Cloud (EC2), to consistently released new product innovations that continue to set AWS’s services and features apart in the industry. As a member of the UC organization, you’ll support the development and management of Compute, Database, Storage, Internet of Things (Iot), Platform, and Productivity Apps services in AWS. Within AWS UC, Amazon Dedicated Cloud (ADC) roles engage with AWS customers who require specialized security solutions for their cloud services.Want to help drive the success of Machine Learning technologies at AWS? Do you have the skills and motivation to build automation that supports next gen ML platforms ? We want to talk to you!We seek a Software Development Engineer for the Machine Learning (ML) Infrastructure team to build the tools that are used to guarantee top performance of AWS ML and High Performance Computing (HPC) technologies developed by our organization. Bring your exceptional knowledge of CI/CD automation, ML and HPC benchmarks and applications to bear on the cutting-edge software we develop. Join us as we expand the AWS offerings for AI, including Trainium, Neuron and the Elastic Fabric Adapter (EFA).Key job responsibilitiesBe the lead engineer on a team that builds and maintains the infrastructure that monitors and reports on functionality and performance of massive testing workloads run at scale. Use internal Amazon CI/CD tools, Linux, and public AWS products to automate the delivery of our software to customers, saving developer time. Write Python code that effortlessly spools up large clusters and runs benchmarks and applications for ML and HPC workloads. Use AWS Managed Grafana, Quicksight, OpenSeaerch  and Athena to digest the massive amount of performance data generated by these workloads and create dashboards for developers and stakeholders. Invent automatic mechanisms to alert developers to functional and performance regressions so they never reach customers. Manage the complexity of infrastructure that covers many instance types, software stacks, Linux operating systems, cutting-edge releases and make it easy to evolve.A day in the lifeEnsure all infrastructure setup is code (IaC), reviewed and committed to automated pipelines. You find innovative ways to schedule work using Jenkins, supporting the development team while keeping cluster costs down. You review dashboard and automation results and triage failures, you introduce new tests and platforms to increase the automation coverage. You create reports and status of the CI/CD system to stakeholders About the teamWe are part of Annapurna Labs, a subsidiary in AWS that builds software and hardware that make ML and HPC  on EC2 work. Our organization is a dedicated group of innovators that have invented new networks, new silicon, new software suites, and combined those to entice customers to move immense ML and HPC workloads to the cloud. The EFA team is laser focused on making AWS the best and most cost-effective place for customers to do run AI and HPC  workload at scale.BASIC QUALIFICATIONS- 3+ years of non-internship professional software development experience- 3+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience- 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience- Bachelor's degree in computer science or equivalent- 3+ years experience coding in Python- Experience developing highly automated CI/CD pipelines (Jenkins preferred)PREFERRED QUALIFICATIONS- Experience creating automated dashboards and visualization (Quicksight, Grafana, OpenSearch)Job detailsISR, HaifaAWS Software Development Software DevelopmentShare this jobJOIN US ONFind CareersJob CategoriesTeamsLocationsUS and EU Military recruitingWarehouse and Hourly JobsWorking At AmazonCultureBenefitsAmazon NewsletterDiversity at AmazonOur leadership principlesHelpFAQInterview tipsReview application statusDisability accommodationsEU background checksAmazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status.Privacy and DataImpressum© 1996-2024, Amazon.com, Inc. or its affiliates\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://www.amazon.jobs/en/jobs/2713915/software-development-engineer-efa\")\n",
    "\n",
    "page_data=loader.load().pop().page_content\n",
    "\n",
    "print(page_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63d107f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"Software Development Engineer, EFA\",\n",
      "        \"experience\": \"3+ years of non-internship professional software development experience, 3+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience, 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience, Bachelor's degree in computer science or equivalent, 3+ years experience coding in Python, Experience developing highly automated CI/CD pipelines (Jenkins preferred)\",\n",
      "        \"skills\": \"CI/CD automation, ML and HPC benchmarks and applications, Python, Jenkins, AWS Managed Grafana, Quicksight, OpenSeaerch, Athena\",\n",
      "        \"description\": \"Be the lead engineer on a team that builds and maintains the infrastructure that monitors and reports on functionality and performance of massive testing workloads run at scale. Use internal Amazon CI/CD tools, Linux, and public AWS products to automate the delivery of our software to customers, saving developer time. Write Python code that effortlessly spools up large clusters and runs benchmarks and applications for ML and HPC workloads. Use AWS Managed Grafana, Quicksight, OpenSeaerch and Athena to digest the massive amount of performance data generated by these workloads and create dashboards for developers and stakeholders. Invent automatic mechanisms to alert developers to functional and performance regressions so they never reach customers. Manage the complexity of infrastructure that covers many instance types, software stacks, Linux operating systems, cutting-edge releases and make it easy to evolve.\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_extract = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        ### SCRAPED TEXT FROM WEBSITE:\n",
    "        {page_data}\n",
    "        ### INSTRUCTION:\n",
    "        The scraped text is from the career's page of a website.\n",
    "        Your job is to extract the job postings and return them in JSON format containing the \n",
    "        following keys: `role`, `experience`, `skills` and `description`.\n",
    "        Only return the valid JSON.\n",
    "        ### VALID JSON (NO PREAMBLE):    \n",
    "        \"\"\"\n",
    ")\n",
    "\n",
    "chain_extract = prompt_extract | llm\n",
    "\n",
    "res=chain_extract.invoke(input={'page_data':page_data})\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "190394e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'Software Development Engineer, EFA', 'experience': \"3+ years of non-internship professional software development experience, 3+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience, 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience, Bachelor's degree in computer science or equivalent, 3+ years experience coding in Python, Experience developing highly automated CI/CD pipelines (Jenkins preferred)\", 'skills': 'CI/CD automation, ML and HPC benchmarks and applications, Python, Jenkins, AWS Managed Grafana, Quicksight, OpenSeaerch, Athena', 'description': 'Be the lead engineer on a team that builds and maintains the infrastructure that monitors and reports on functionality and performance of massive testing workloads run at scale. Use internal Amazon CI/CD tools, Linux, and public AWS products to automate the delivery of our software to customers, saving developer time. Write Python code that effortlessly spools up large clusters and runs benchmarks and applications for ML and HPC workloads. Use AWS Managed Grafana, Quicksight, OpenSeaerch and Athena to digest the massive amount of performance data generated by these workloads and create dashboards for developers and stakeholders. Invent automatic mechanisms to alert developers to functional and performance regressions so they never reach customers. Manage the complexity of infrastructure that covers many instance types, software stacks, Linux operating systems, cutting-edge releases and make it easy to evolve.'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "parsed_data=json.loads(res.content)\n",
    "\n",
    "if isinstance(parsed_data,list) and len(parsed_data)==1:\n",
    "    parsed_data=parsed_data[0]\n",
    "\n",
    "print(parsed_data)\n",
    "print(type(parsed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83627e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'Software Development Engineer, EFA', 'experience': \"3+ years of non-internship professional software development experience, 3+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience, 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience, Bachelor's degree in computer science or equivalent, 3+ years experience coding in Python, Experience developing highly automated CI/CD pipelines (Jenkins preferred)\", 'skills': 'CI/CD automation, ML and HPC benchmarks and applications, Python, Jenkins, AWS Managed Grafana, Quicksight, OpenSeaerch, Athena', 'description': 'Be the lead engineer on a team that builds and maintains the infrastructure that monitors and reports on functionality and performance of massive testing workloads run at scale. Use internal Amazon CI/CD tools, Linux, and public AWS products to automate the delivery of our software to customers, saving developer time. Write Python code that effortlessly spools up large clusters and runs benchmarks and applications for ML and HPC workloads. Use AWS Managed Grafana, Quicksight, OpenSeaerch and Athena to digest the massive amount of performance data generated by these workloads and create dashboards for developers and stakeholders. Invent automatic mechanisms to alert developers to functional and performance regressions so they never reach customers. Manage the complexity of infrastructure that covers many instance types, software stacks, Linux operating systems, cutting-edge releases and make it easy to evolve.'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "json_par=JsonOutputParser()\n",
    "json_res=json_par.parse(res.content[0])\n",
    "print(json_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bc50ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(json_res))\n",
    "print(type(res.content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
